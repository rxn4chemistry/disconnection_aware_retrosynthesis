{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from rxn.chemutils.reaction_smiles import parse_any_reaction_smiles, ReactionEquation\n",
    "from rxn.chemutils.tokenization import tokenize_smiles\n",
    "from rxn.chemutils.multicomponent_smiles import canonicalize_multicomponent_smiles\n",
    "from rxn.chemutils.reaction_equation import canonicalize_compounds, sort_compounds\n",
    "\n",
    "from rxn.chemutils.utils import remove_atom_mapping\n",
    "\n",
    "from dar.tagging import get_tagged_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_agents_as_reactants(reaction_smiles: str) -> str:\n",
    "    \"\"\"\n",
    "    Rearranges reaction SMILES by moving agents to the reactants.\n",
    "\n",
    "    Args:\n",
    "        reaction_smiles: Reaction SMILES string\n",
    "    Returns:\n",
    "        Reaction SMILES with agents cast as reactants.\n",
    "    \"\"\"\n",
    "\n",
    "    rxn = parse_any_reaction_smiles(reaction_smiles)\n",
    "    rxn.reactants.extend(rxn.agents)\n",
    "    rxn.agents = []\n",
    "    \n",
    "    return rxn.to_string()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_reaction(reaction_smiles: str, remove_atom_maps: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Canonicalises and sorts compounds. Optionally Removes atom mapping.\n",
    "\n",
    "    Args:\n",
    "        reaction_smiles: Reaction SMILES string\n",
    "        remove_atom_mapping: Optionally remove atom \n",
    "    Returns:\n",
    "        Reaction SMILES with agents cast as reactants.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if remove_atom_maps:\n",
    "            reaction = remove_atom_mapping(reaction_smiles)\n",
    "        reaction = parse_any_reaction_smiles(reaction)\n",
    "\n",
    "        reaction = canonicalize_compounds(reaction)\n",
    "        reaction = sort_compounds(reaction)\n",
    "\n",
    "        return reaction.to_string()\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging Products from Reaction SMILES\n",
    "As an example we demonstrate a basic pre-processing workflow for tagging products from a reaction SMILES. We leave the user to carry out their own pre-processing and cleaning steps as suits their needs. We start from the USPTO dataset which contain atom-mapped reaction SMILES, we recommend remapping the reaction SMILES with RXNMapper or an atom-mapping tool of your choice. Here we use the '1976_Sep2016_USPTOgrants_smiles.rsmi' obtained from unzipping '1976_Sep2016_USPTOgrants_smiles.7z', which can be found at:  \n",
    "\n",
    "[US Patent Office extracts (USPTO) by Lowe] (https://figshare.com/articles/dataset/Chemical_reactions_from_US_patents_1976-Sep2016_/5104873)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('1976_Sep2016_USPTOgrants_smiles.rsmi', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic pre-processing\n",
    "\n",
    "Note: the pre-processing shown here is a simplified version of that used in the manuscript. Please refer to the SI for a full list of preprocessing steps or adapt as needed to your problem. **Duplicates will remain in the pre-processing below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ReactionSmiles\"] = df[\"ReactionSmiles\"].apply(cast_agents_as_reactants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split reactions into reactants and products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"reactants\", \"products\"]] = df[\"ReactionSmiles\"].str.split(\">>\", expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"number_of_products\"] = df[\"products\"].apply(lambda product: len(product.split('.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"number_of_reactants\"] = df[\"reactants\"].apply(lambda product: len(product.split('.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[(df[\"number_of_reactants\"]>= 2) & (df[\"number_of_products\"] == 1)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[\"ReactionSmiles\"] = filtered_df[\"ReactionSmiles\"].apply(standardise_reaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df[filtered_df[\"ReactionSmiles\"] != \"\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.drop_duplicates(subset=[\"ReactionSmiles\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tag products "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[\"tagged_products\"] = filtered_df.apply(lambda x: get_tagged_products(x['reactants'], x['products']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[[\"reactants\", \"products\"]] = filtered_df[\"ReactionSmiles\"].str.split(\">>\", expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenise samples for training disconnection aware retrosynthesis model and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[\"reactants\"] = filtered_df[\"reactants\"].apply(tokenize_smiles)\n",
    "filtered_df[\"tagged_products\"] = filtered_df[\"tagged_products\"].apply(tokenize_smiles)\n",
    "\n",
    "#filtered_df['reactants'].to_csv('precursors_tokens.txt', index=False, header=False)\n",
    "#filtered_df['tagged_products'].to_csv('tagged_products_tokens.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare for training model for automatic identification of disconnection sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[\"products\"] = filtered_df[\"products\"].apply(tokenize_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_df['products'].to_csv('products_tokens.txt', index=False, header=False)\n",
    "#filtered_df['tagged_products'].to_csv('tagged_products_tokens.txt', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a8fffb9b41741f41ef95be7e4f9c74dffdbe0f08fc2be86c213456ec0cc01bf4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
